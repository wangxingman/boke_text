

## K8S概述

### 	k8s是什么

Kubernetes是容器集群管理系统，是一个开源的平台，可以实现容器集群的自动化部署、自动扩缩容、维护等功能。

#### 	k8s特点

- **可移植**: 支持公有云，私有云，混合云，多重云（multi-cloud）
- **可扩展**: 模块化, 插件化, 可挂载, 可组合
- **自动化**: 自动部署，自动重启，自动复制，自动伸缩/扩展



### 	为何使用容器

![为什么是容器？](https://d33wubrfki0l68.cloudfront.net/e7b766e0175f30ae37f7e0e349b87cfe2034a1ae/3e391/images/docs/why_containers.svg)

传统的应用部署方式是通过插件或脚本来安装应用。这样做的缺点是应用的运行、配置、管理、所有生存周期将与当前操作系统绑定，这样做并不利于应用的升级更新/回滚等操作，当然也可以通过创建虚机的方式来实现某些功能，但是虚拟机非常重，并不利于可移植性。

新的方式是通过部署容器方式实现，每个容器之间互相隔离，每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。相对于虚拟机，容器能快速部署，由于容器与底层设施、机器文件系统解耦的，所以它能在不同云、不同版本操作系统间进行迁移。

容器占用资源少、部署快，每个应用可以被打包成一个容器镜像，每个应用与容器间成一对一关系也使容器有更大优势，使用容器可以在build或release 的阶段，为应用创建容器镜像，因为每个应用不需要与其余的应用堆栈组合，也不依赖于生产环境基础结构，这使得从研发到测试、生产能提供一致环境。类似地，容器比虚机轻量、更“透明”，这更便于监控和管理。最后，

容器优势总结：

- **快速创建/部署应用：**与VM虚拟机相比，容器镜像的创建更加容易。
- **持续开发、集成和部署：**提供可靠且频繁的容器镜像构建/部署，并使用快速和简单的回滚(由于镜像不可变性)。
- **开发和运行相分离：**在build或者release阶段创建容器镜像，使得应用和基础设施解耦。
- **开发，测试和生产环境一致性：**在本地或外网（生产环境）运行的一致性。
- **云平台或其他操作系统：**可以在 Ubuntu、RHEL、 CoreOS、on-prem、Google Container Engine或其它任何环境中运行。
- **Loosely coupled，分布式，弹性，微服务化：**应用程序分为更小的、独立的部件，可以动态部署和管理。
- **资源隔离**
- **资源利用：**更高效

​	

## K8S架构

### 整体架构

![architecture](https://feisky.gitbooks.io/kubernetes/architecture/images/architecture.png)

![img](https://images2015.cnblogs.com/blog/1087716/201704/1087716-20170411111532391-899038129.png)



### 主节点

![img](https://feisky.gitbooks.io/kubernetes/architecture/images/14791969222306.png)



​	k8s集群的管理节点，负责管理集群，提供集群的资源数据访问入口。拥有Etcd存储服务（可选），运行Api Server进程，Controller Manager服务进程及Scheduler服务进程，关联工作节点Node。Kubernetes API server提供HTTP Rest接口的关键服务进程，是Kubernetes里所有资源的增、删、改、查等操作的唯一入口。也是集群控制的入口进程；Kubernetes Controller Manager是Kubernetes所有资源对象的自动化控制中心；Kubernetes Schedule是负责资源调度（Pod调度）的进程



- **etcd**保存了整个集群的状态；
- **apiserver**提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制；
- **controller manager**负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；
- **scheduler**负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上；



状态 进入 调度 管理

### 从节点

![img](https://feisky.gitbooks.io/kubernetes/architecture/images/14791969311297.png)

​	Node是Kubernetes集群架构中运行Pod的服务节点（亦叫agent或minion）。Node是Kubernetes集群操作的单元，用来承载被分配Pod的运行，是Pod运行的宿主机。关联Master管理节点，拥有名称和IP、系统资源信息。运行docker eninge服务，守护进程kunelet及负载均衡器kube-proxy.

　　Node节点可以在运行期间动态增加到Kubernetes集群中，默认情况下，kubelet会想master注册自己，这也是Kubernetes推荐的Node管理方式，kubelet进程会定时向Master汇报自身情报，如操作系统、Docker版本、CPU和内存，以及有哪些Pod在运行等等，这样Master可以获知每个Node节点的资源使用情况，冰实现高效均衡的资源调度策略。、





- **kubelet**负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理；
- **Container runtime**负责镜像管理以及Pod和容器的真正运行（CRI）；
- **kube-proxy**负责为Service提供cluster内部的服务发现和负载均衡；

周期 运行 负载



### 其他

- **kube-dns**负责为整个集群提供DNS服务
- **Ingress Controller**为服务提供外网入口
- **Heapster**提供资源监控
- **Dashboard**提供GUI
- **Federation**提供跨可用区的集群
- **Fluentd-elasticsearch**提供集群日志采集、存储与查询



## K8S组件

<!--上图组件的详细介绍-->

### Master组件

#### 	介绍

Master组件提供集群的管理控制中心。

#### 	kube-apiserver

<u>**kube-apiserver**</u>用于暴露Kubernetes API。任何的资源请求/调用操作都是通过kube-apiserver提供的接口进行。

#### 	ETCD

<u>**etcd**</u>是Kubernetes提供默认的存储系统，保存所有集群数据，使用时需要为etcd数据提供备份计划。

####      kube-controller-manager

 **<u>kube-controller-manager</u>**运行管理控制器，它们是集群中处理常规任务的后台线程。逻辑上，每个控制器是一个单独的进程，但为了降低复杂性，它们都被编译成单个二进制文件，并在单个进程中运行

​	

- [节点（Node）控制器]
- 副本（Replication）控制器：负责维护系统中每个副本中的pod。
- 端点（Endpoints）控制器：填充Endpoints对象（即连接Services＆Pods）。
- [Service Account]和Token控制器：为新的[Namespace]创建默认帐户访问API Token。



#### 	cloud-controller-manager

云控制器管理器负责与底层云提供商的平台交互。

- 节点（Node）控制器
- 路由（Route）控制器
- Service控制器
- 卷（Volume）控制器



#### 	kube-scheduler

kube-scheduler 监视新创建没有分配到[Node]的[Pod])，为Pod选择一个Node。



#### 	插件 addons		

插件（addon）是实现集群pod和Services功能的

##### 		DNS

能够为 Kubernetes services提供 DNS记录。

##### 		用户界面

kube-ui提供集群状态基础信息查看

##### 		容器资源监测

提供一个UI浏览监控数据

##### 		Cluster-level Logging

负责保存容器日志，搜索/查看日志



### Node组件

#### 	介绍

节点组件运行在[Node]，提供Kubernetes运行时环境，以及维护Pod。

#### 	kubelet

负责对Pod对于的容器的创建、启停等任务

**<u>[kubelet]</u>**是主要的节点代理，它会监视已分配给节点的pod

- 安装Pod所需的volume。
- 下载Pod的Secrets。
- Pod中运行的 docker（或experimentally，rkt）容器。
- 定期执行容器健康检查。



#### 	kube-proxy

实现Kubernetes Service的通信与负载均衡机制的重要组件

**<u>[kube-proxy]</u>**通过在主机上维护网络规则并执行连接转发来实现Kubernetes服务抽象。





#### 	docker

docker用于运行容器。



#### 	RKT

rkt运行容器，作为docker工具的替代方案。



#### 	supervisord

supervisord是一个轻量级的监控系统，用于保障kubelet和docker运行。



#### 	fluentd

fluentd是一个守护进程，可提供[cluster-level logging].。



## K8S Namespace概述

​	**<u>Namespace**</u>是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的pods, services, replication controllers和deployments等都是属于某一个namespace的（默认是default），而node, persistentVolumes等则不属于任何namespace。

​	**<u>Namespace**</u>常用来隔离不同的用户，比如Kubernetes自带的服务一般运行在kube-system namespace中。

#### 查询

```
 kubectl get namespaces
```

#### 创建

```
(1) 命令行直接创建
$ kubectl create namespace new-namespace

(2) 通过文件创建
$ cat my-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: new-namespace

$ kubectl create -f ./my-namespace.yaml
```

#### 删除

```
$ kubectl delete namespaces new-namespace
```



## K8S Labels概述

​	<u>**Labels**</u>其实就一对 key/value ，被关联到对象上，标签的使用我们倾向于能够标示对象的特殊特点，并且对用户而言是有意义的（就是一眼就看出了这个Pod是尼玛数据库），但是标签对内核系统是没有直接意义的。标签可以用来划分特定组的对象（比如，所有女的），标签可以在创建一个对象的时候直接给与，也可以在后期随时修改，每一个对象可以拥有多个标签，但是，key值必须是唯一的

```
"labels": {
  "key1" : "value1",
  "key2" : "value2"
}
```



## K8S Volume

<!-- 简单理解为挂在一个目录-->

### 产生原因

​	默认情况下容器中的磁盘文件是非持久化的，对于运行在容器中的应用来说面临两个问题，第一：当容器挂掉kubelet将重启启动它时，文件将会丢失；第二：当[Pod]中同时运行多个容器，容器之间需要共享文件时。Kubernetes的Volume解决了这两个问题



### 概述

​	Volume具有明确的生命周期 - 与pod相同。因此，Volume的生命周期比Pod中运行的任何容器要持久，在容器重新启动时能可以保留数据，当然，当Pod被删除不存在时，Volume也将消失。注意，Kubernetes支持许多类型的Volume，Pod可以同时使用任意类型/数量的Volume。

内部实现中，一个Volume只是一个目录，目录中可能有一些数据，pod的容器可以访问这些数据。至于这个目录是如何产生的、支持它的介质、其中的数据内容是什么，这些都由使用的特定Volume类型来决定。

要使用Volume，pod需要指定Volume的类型和内容（`spec.volumes`字段），和映射到容器的位置（`spec.containers.volumeMounts`字段）。



## K8S Pod概述

### 概述

Pod是Kubernetes创建或部署的最小/最简单的基本单位，一个Pod代表集群上正在运行的一个进程。

一个Pod封装一个应用容器（也可以有多个容器），存储资源、一个独立的网络IP以及管理控制容器运行方式的策略选项。Pod代表部署的一个单位：Kubernetes中单个应用的实例，它可能由单个容器或多个容器共享组成的资源。

Pod是Kubernetes创建或部署的最小/最简单的基本单位，一个Pod代表集群上正在运行的一个进程。

一个Pod封装一个应用容器（也可以有多个容器），存储资源、一个独立的网络IP以及管理控制容器运行方式的策略选项。Pod代表部署的一个单位：Kubernetes中单个应用的实例，它可能由单个容器或多个容器共享组成的资源。

### Pods如何管理多个容器

Pods的设计可用于支持多进程的协同工作（作为容器），形成一个cohesive的Service单位。Pod中的容器在集群中Node上被自动分配，容器之间可以共享资源、网络和相互依赖关系，并同时被调度使用。

### 网络

每个Pod被分配一个独立的IP地址，Pod中的每个容器共享网络命名空间，包括IP地址和网络端口。Pod内的容器可以使用localhost相互通信。当Pod中的容器与Pod 外部通信时，他们必须协调如何使用共享网络资源（如端口）。

#### 存储

Pod可以指定一组共享存储*volumes*。Pod中的所有容器都可以访问共享*volumes*，允许这些容器共享数据。*volumes* 还用于Pod中的数据持久化，以防其中一个容器需要重新启动而丢失数据

### 使用Pod

Pod不会自愈。如果Pod运行的Node故障，或者是调度器本身故障，这个Pod就会被删除。同样的，如果Pod所在Node缺少资源或者Pod处于维护状态，Pod也会被驱逐。Kubernetes使用更高级的称为Controller的抽象层，来管理Pod实例。虽然可以直接使用Pod，但是在Kubernetes中通常是使用Controller来管理Pod的。

#### Pod和Controller

Controller可以创建和管理多个Pod，提供副本管理、滚动升级和集群级别的自愈能力。例如，如果一个Node故障，Controller就能自动将该节点上的Pod调度到其他健康的Node上。

包含一个或者多个Pod的Controller示例：

·     [Deployment]

·     [StatefulSet]

·     DaemonSet

 

 

   **<u>[nginx-deployment.yaml ]</u>**

```
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
```



```
kubectl create -f nginx-deployment.yaml --record
```

## Replication Controller

　　Replication Controller用来管理Pod的副本，保证集群中存在指定数量的Pod副本。集群中副本的数量大于指定数量，则会停止指定数量之外的多余容器数量，反之，则会启动少于指定数量个数的容器，保证数量不变。Replication Controller是实现弹性伸缩、动态扩容和滚动升级的核心。



## K8S Nodes概述

​	Node是Kubernetes中的工作节点，最开始被称为minion。一个Node可以是VM或物理机。每个Node（节点）具有运行[pod](http://docs.kubernetes.org.cn/312.html)的一些必要服务，并由Master组件进行管理，Node节点上的服务包括Docker、kubelet和kube-proxy。

### 节点状态

1、**Addresses**

这些字段的使用取决于云提供商或裸机配置。

2、**Condition**

conditions字段描述所有Running节点的状态。

3、**Capacity**

描述节点上可用的资源：CPU、内存和可以调度到节点上的最大pod数。

4、**Info**

关于节点的一些基础信息，如内核版本、Kubernetes版本（kubelet和kube-proxy版本）、Docker版本（如果有使用）、OS名称等。信息由Kubelet从节点收集。



### 创建节点

​		它是由Google Compute Engine等云提供商在外部创建的，或使用物理和虚拟机。这意味着当Kubernetes创建一个节点时，它只是创建一个代表节点的对象，创建后，Kubernetes将检查节点是否有效。

```
{
  "kind": "Node",
  "apiVersion": "v1",
  "metadata": {
    "name": "10.240.79.157",
    "labels": {
      "name": "my-first-k8s-node"
    }
  }
}
```

​	Kubernetes将在内部创建一个节点对象，并通过基于metadata.name字段的健康检查来验证节点，如果节点有效，即所有必需的服务会同步运行，则才能在上面运行pod。请注意，Kubernetes将保留无效节点的对象（除非客户端有明确删除它）并且它将继续检查它是否变为有效。



### Node Controller

节点控制器（Node Controller）是管理节点的Kubernetes master组件。

节点控制器在节点的生命周期中具有多个角色。



### Node容量

节点的容量(cpu数量和内存数量)是节点对象的一部分。

通常，节点在创建节点对象时注册并通知其容量。如果是[手动管理节点]，则需要在添加节点时设置节点容量。



## K8S Service概述

### 概述

　　Service定义了Pod的逻辑集合和访问该集合的策略，是真实服务的抽象。Service提供了一个统一的服务访问入口以及服务代理和发现机制，关联多个相同Label的Pod，用户不需要了解后台Pod是如何运行。

Kubernetes Service 定义了这样一种抽象：一个 Pod 的逻辑分组，一种可以访问它们的策略 —— 通常称为微服务。 这一组 Pod 能够被 Service 访问到，通常是通过 [Label Selector]

### 定义 Service

一个 Service 在 Kubernetes 中是一个 REST 对象，和 Pod 类似。 像所有的 REST 对象一样， Service 定义可以基于 POST 方式，请求 apiserver 创建新的实例。 例如，假定有一组 Pod，它们对外暴露了 9376 端口，同时还被打上 "app=MyApp" 标签。

 

```
kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
```

​    targetPort: 9376



## 





## Kubernetes UI

Kubernetes有一个基于web的用户界面，它可以图表化显示当前集群状态。

### 节点资源使用

![20161116100748](https://img.kubernetes.org.cn/2016/11/20161116100748.jpg)

### Dashboard视图

在页面的右上方点击“Views”按钮可以看到其他可用的视图，包括Explore，Pods，Nodes，Replication Controllers，Services和Events。

### Explore视图

Explore视图允许你轻松看到当前集群的pods，replication controller和services。

![20161116102030](https://img.kubernetes.org.cn/2016/11/20161116102030.jpg)

“Group by”下拉列表允许你用类型、名称、主机等条件对资源分组。

 

![20161116102038](https://img.kubernetes.org.cn/2016/11/20161116102038.jpg)

 

你也可以点击资源列表的下拉三角来生成一个过滤器，有多重过滤类型可供选择。

 

![20161116102050](https://img.kubernetes.org.cn/2016/11/20161116102050.jpg)

 

若是要看每项资源的更多细节，就在上面点击。

 

![20161116102059](https://img.kubernetes.org.cn/2016/11/20161116102059.jpg)

 

### 其他视图

其他视图（Pods, Nodes, Replication Controllers, Services, and Events）简单列出了每种资源的信息。你也可以点击其他种类获取更多细节。

 

![20161116102113](https://img.kubernetes.org.cn/2016/11/20161116102113.jpg)



## Kubernetes kubectl 概述

### 概述

kubectl用于运行Kubernetes集群命令的管理工具。

### 命令

#### kubectl run

##### 概述

创建并运行一个或多个容器镜像。

创建一个deployment 或job 来管理容器。

##### **语法**

```
 run NAME --image=image [--env="key=value"] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [--command] -- [COMMAND] [args...]
```

##### 实例

启动nginx实例。

```shell
kubectl run nginx --image=nginx --env="DNS_DOMAIN=cluster" --env="POD_NAMESPACE=default" --port=5701 --replicas=5 --dry-run
```

run  运行

image 要运行的容器的映像。

env 设置环境变量

port 映射端口

replicas 设置副本数量

--dry-run  打印相应的API对象而不创建它们



#### kubectl expose

##### 概述

将资源暴露为新的Kubernetes Service。

##### 语法

```
 expose (-f FILENAME | TYPE NAME) [--port=port] [--protocol=TCP|UDP] [--target-port=number-or-name] [--name=name] [--external-ip=external-ip-of-service] [--type=type]
```

##### 实例

为RC的nginx创建service，并通过Service的80端口转发至容器的8000端口上。

```
kubectl expose rc nginx --port=80 --target-port=8000
```

由“nginx-controller.yaml”中指定的type和name标识的RC创建Service，并通过Service的80端口转发至容器的8000端口上。

```
kubectl expose -f nginx-controller.yaml --port=80 --target-port=8000
```

#### kubectl create

##### 概述

通过配置文件名或stdin创建一个集群资源对象。

支持JSON和YAML格式的文件。

##### 语法

```
create -f FILENAME
```

##### 实例

通过pod.json文件创建一个pod。

```
kubectl create -f ./pod.json
```

通过stdin的JSON创建一个pod。

```
cat pod.json | kubectl create -f -
```

API版本为v1的JSON格式的docker-registry.yaml文件创建资源。

```
kubectl create -f docker-registry.yaml --edit --output-version=v1 -o json
```



#### kubectl **delete** 

##### 概述

通过配置文件名、stdin、资源名称或label选择器来删除资源。

##### 语法

```
$ delete ([-f FILENAME] | TYPE [(NAME | -l label | --all)])
```

##### 实例

使用 pod.json中指定的资源类型和名称删除pod。

```
kubectl delete -f ./pod.json
```

根据传入stdin的JSON所指定的类型和名称删除pod。

```
cat pod.json | kubectl delete -f -
```

删除名为“baz”和“foo”的Pod和Service。

```
kubectl delete pod,service baz foo
```

删除 Label name = myLabel的pod和Service。

```
kubectl delete pods,services -l name=myLabel
```

强制删除dead node上的pod

```
kubectl delete pod foo --grace-period=0 --force
```

删除所有pod

```
kubectl delete pods --all
```



#### kubectl get

##### 概述

获取列出一个或多个资源的信息。

##### 语法

```
 get [(-o|--output=)json|yaml|wide|custom-columns=...|custom-columns-file=...|go-template=...|go-template-file=...|jsonpath=...|jsonpath-file=...] (TYPE [NAME | -l label] | TYPE/NAME ...) [flags]
```

##### 实例

列出所有运行的Pod信息。

```
kubectl get pods
```

列出Pod以及运行Pod节点信息。

```
kubectl get pods -o wide
```

列出指定NAME的 replication controller信息。

```
kubectl get replicationcontroller web
```

以JSON格式输出一个pod信息。

```
kubectl get -o json pod web-pod-13je7
```

以“pod.yaml”配置文件中指定资源对象和名称输出JSON格式的Pod信息。

```
kubectl get -f pod.yaml -o json
```

返回指定pod的相位值。

```
kubectl get -o template pod/web-pod-13je7 --template={{.status.phase}}
```

列出所有replication controllers和service信息。

```
kubectl get rc,services
```

按其资源和名称列出相应信息。

```
kubectl get rc/web service/frontend pods/web-pod-13je7
```

列出所有不同的资源对象。

```
kubectl get all
```



#### kubectl scale

##### 概述

扩容或缩容 Deployment、ReplicaSet、Replication Controller或 Job 中Pod数量

##### 语法

```
scale [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME)
```

##### 实例

将名为foo中的pod副本数设置为3。

```
kubectl scale --replicas=3 rs/foo
```

将由“foo.yaml”配置文件中指定的资源对象和名称标识的Pod资源副本设为3。

```
kubectl scale --replicas=3 -f foo.yaml
```

如果当前副本数为2，则将其扩展至3。

```
kubectl scale --current-replicas=2 --replicas=3 deployment/mysql
```

设置多个RC中Pod副本数量。

```
kubectl scale --replicas=5 rc/foo rc/bar rc/baz
```



## K8S搭建

### 单机版

https://www.jianshu.com/p/d27141e18398

#### 安装docker

```cpp
// 安装docker
$ yum install -y docker-ce
// 开机启动 && 启动服务
$ systemctl enable docker && systemctl start docker

systemctl disable --now firewalld  # 关闭防火墙
```

#### 安装kubelet kubeadm kubectl

<!--安装kubernetes的时候，需要安装kubelet, kubeadm等包，但k8s官网给的yum源是packages.cloud.google.com，国内访问不了，此时我们可以使用阿里云的yum仓库镜像。-->

在/etc/yum.repos.d/创建**kubernetes.repo**

```
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
```

​    sysctl --system  # 重新加载所有配置文件

**关闭SElinux**

```
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
```

**安装kubelet kubeadm kubectl**

```
yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
```

**开机启动kubelet**

```
systemctl enable kubelet  && systemctl start kubelet
```

#### 拉取镜像

```php
// 查看kubeadm镜像
$ kubeadm config images list

// 结果
k8s.gcr.io/kube-apiserver:v1.13.3
k8s.gcr.io/kube-controller-manager:v1.13.3
k8s.gcr.io/kube-scheduler:v1.13.3
k8s.gcr.io/kube-proxy:v1.13.3
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd:3.2.24
k8s.gcr.io/coredns:1.2.6

// 执行如下脚本（没有翻墙的同学只能通过阿里云镜像或者其他镜像）
$ for i in `kubeadm config images list`; do 
  imageName=${i#k8s.gcr.io/}
  docker pull registry.aliyuncs.com/google_containers/$imageName
  docker tag registry.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName
  docker rmi registry.aliyuncs.com/google_containers/$imageName
done;

// 开机启动 && 启动服务
$ systemctl enable kubelet && systemctl start kubelet
```

#### 初始化

```kotlin
// 安装命令
$ kubeadm init
```



#### 问题

```csharp
[ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1

// 解决：
// 编辑配置
$ vi /etc/sysctl.conf
// 添加如下内容
net.bridge.bridge-nf-call-iptables = 1
```



```csharp
[ERROR Swap]: running with swap on is not supported. Please disable swap

// 解决：
// 禁用swap功能
$ swapoff -a

// 修改配置
$ vi /etc/fstab
# 注释如下内容
# k8s need disabled
# /dev/mapper/centos-swap swap                    swap    defaults        0 0
```

#### 安装成功

```undefined
Your Kubernetes master has initialized successfully!
```

后面配置

```php

// 安装成功后根据提示配置
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
// master 参与工作（单机模式必备）
kubectl taint nodes --all node-role.kubernetes.io/master-
```



查看k8s集群情况(现在只有system pod)

```csharp
 kubectl get pods --all-namespaces
```

开启单机模式

```bash
$kubectl taint nodes --all node-role.kubernetes.io/master-
```

查看master节点情况

```csharp
kubectl get nodes
```

安装网络插件

```cpp
 kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
```

#### 配置tomcat RC

```cpp
// 查看所需要镜像
docker search tomcat
// 下载tag为tomcat的images（默认版本为lasted）
docker pull tomcat
```

> `replicas: 1` pod实例个数为1
>  `image: tomcat` docker镜像
>  `name: tomcat-demo` rc名称
>  `spec:template:` 当运行实例个数小于replicas时候，rc会根据spec:template: 自动生成对应个数pod



```cpp
apiVersion: v1
kind: ReplicationController
metadata:
  name: tomcat-demo
spec:
  replicas: 1
  selector:
    app: tomcat-demo
  template:
    metadata:
      labels:
        app: tomcat-demo
    spec:
      containers:
      - name: tomcat-demo
        image: tomcat
        ports:
        - containerPort: 8080
```

运行配置并查看结果



```swift
// 运行yaml
$ kubectl create -f tomcat-demo-rc.yaml
// 结果如下
tomcat-demo   NodePort    10.105.57.5   <none>        8080:30001/TCP   14s
```

#### 配置tomcat service

> `nodePort: 30001` 映射端口8080:30001
>  `name: tomcat-demo` 服务名

`tomcat-demo-svc.yaml`配置文件内容



```bash
apiVersion: v1
kind: Service
metadata:
  name: tomcat-demo
spec:
  type: NodePort
  ports:
   - port: 8080
     nodePort: 30001
  selector:
    app: tomcat-demo
```

运行配置并查看结果



```swift
// 运行yaml
$ kubectl create -f tomcat-demo-svc.yaml
// 结果如下
tomcat-demo   NodePort    10.105.57.5   <none>        8080:30001/TCP   14s
```

> `注意` svc与rc文件可以写在同一个yaml中
>  开通端口号



```csharp
$ firewall-cmd --zone=public --add-port=30001/tcp --permanent && firewall-cmd --reload
```



浏览器中查看结果 `http://${ip地址}:30001/`



### 集群版

https://www.kubernetes.org.cn/3096.html

https://segmentfault.com/a/1190000018905869



### k8sUI页面

https://www.jianshu.com/p/76eda0a82540



## 学习资源

**csdn**

https://www.jianshu.com/p/9d52a32f648d

https://blog.csdn.net/weixin_43277643/article/details/83382532

**官网文档**

http://docs.kubernetes.org.cn/230.html

**k8s和docker的关系**

https://www.jianshu.com/p/f1f94c6968f5

**部署java程序到k8s**

https://blog.csdn.net/qq_22917163/article/details/84571235

https://www.jianshu.com/p/838315589db4

**微信公众号**

 https://mp.weixin.qq.com/s?__biz=MzU1Nzg4NjgyMw==&mid=2247484180&idx=1&sn=5bcbbc7dc4c5ef7e561712caa92d6e4b&chksm=fc2fbf1ccb58360a974f1776e71f650889ac99947b1b2a7683c9c82574c1dbabf13ec3ef65b0&mpshare=1&scene=23&srcid=&sharer_sharetime=1574645598496&sharer_shareid=dc65c4c228810dbe236d1b5d0456b54e#rd 

