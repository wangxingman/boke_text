

## Hadoop简介

### 大数据介绍

简单说就是 **数据处理**
在互联网技术发展到现今阶段，大量日常、工作等事务产生的数据都已经信息化，人类产生的数据量相比以前有了爆炸式的增长，以前的传统的数据处理技术已经无法胜任，需求催生技术，一套用来处理海量数据的软件工具应运而生，这就是大数据！



### hadoop概述

Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，则MapReduce为海量的数据提供了计算。



### 为什么要用Hadoop

```go
* 海量数据需要及时分析和处理
* 海量数据需要深入分析和挖掘
* 数据需要长期保存
* 海量数据存储的问题：
* 磁盘IO称为一种瓶颈，而非CPU资源
* 网络带宽是一种稀缺资源
* 硬件故障成为影响稳定的一大因素
```



## Hadoop架构

![img](https://pennywong.gitbooks.io/hadoop-notebook/content/pic/architecture.png)

- **HDFS:** 分布式文件存储
- **YARN:** 分布式资源管理
- **MapReduce:** 分布式计算
- **Others:** 利用YARN的资源管理功能实现其他的数据处理方式



## Hadoop组件

### HDFS

#### 简介

​	HDFS是分布式文件系统。HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS放宽了一部分POSIX约束，来实现流式读取文件系统数据的目的

#### 优点

##### 容错性

错误检测和快速、自动的恢复是HDFS最核心的架构目标。

##### 流式数据访问

运行在HDFS上的应用和普通的应用不同，需要流式访问它们的数据集，提高吞吐量。

##### 海量存储

运行在HDFS上的应用具有很大的数据集。HDFS上的一个典型文件大小一般都在G字节至T字节。

##### 文件分块存储

​    HDFS 将一个完整的大文件平均分块（通常每块（block）128M）存储到不同计算机上，这样读取文件可以同时从多个主机取不同区块的文件，多主机读取比单主机读取效率要高得多得多。  <u>block的大小，我们实际可以自己控制！</u>

##### 简单的一致性模型

一次写入多次读取”的文件访问模型。一个文件经过创建、写入和关闭之后就不需要改变。这一假设简化了数据一致性问题，并且使高吞吐量的数据访问成为可能。

##### 移动计算

​    在数据存储的地方进行计算，而不是把数据拉取到计算的地方，降低了成本，提高了性能！  

##### 低成本

 通过多副本提高可靠性，提供了容错和恢复机制。HDFS 可以应用在普通PC 机上，这种机制能够让一些公司用几十台廉价的计算机就可以撑起一个大数据集群。



#### 架构

![HDFS 架构](http://hadoop.apache.org/docs/r1.0.4/cn/images/hdfsarchitecture.gif)

**<u>HDFS</u>** 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成:

1. **Namenode**：负责管理文件系统的名字空间(namespace)以及客户端对文件的访问
2. **Datanode**：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作
3. **Block**:所有的文件都是以 block 块的方式存放在 HDFS 文件系统当中



#### **<u>NameNode</u>**

#### 文件系统的名字空间

HDFS 的 `文件系统命名空间` 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。`NameNode` 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。

#### 数据复制

##### 介绍

![img](https://raw.githubusercontent.com/heibaiying/BigData-Notes/master/pictures/hdfsdatanodes.png)

由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列<u>**块**</u>，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，<u>默认复制因子是 3</u>）。



##### 实现原理

大型的 HDFS 实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，同一机架中的服务器间的网络带宽大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是：

在写入程序位于 `datanode` 上时，就优先将写入文件的一个副本放置在该 `datanode` 上，否则放在随机 `datanode` 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。

[![img](https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hdfs-%E6%9C%BA%E6%9E%B6.png)](https://github.com/heibaiying/BigData-Notes/blob/master/pictures/hdfs-机架.png)

如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 `（复制系数 - 1）/机架数量 + 2`，需要注意的是不允许同一个 `dataNode` 上具有同一个块的多个副本。



#### 副本的选择

为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。



####  架构的稳定性

#####  心跳机制和重新复制

每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。



#####  数据的完整性

由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下：

当客户端创建 HDFS 文件时，它会计算文件的每个块的 `校验和`，并将 `校验和` 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 `校验和` 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。



#####  元数据的磁盘故障

`FsImage` 和 `EditLog` 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 `FsImage` 和 `EditLog` 多副本同步，这样 `FsImage` 或 `EditLog` 的任何改变都会引起每个副本 `FsImage` 和 `EditLog` 的同步更新。



#####  支持快照

快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。



#### <u>DataNode</u>

##### HDFS写数据原理

![img](https://img-blog.csdn.net/20140710141109143?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHVkaWVmZW5tdQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)![img](https://img-blog.csdn.net/20140710141631268?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHVkaWVmZW5tdQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)



![img](https://img-blog.csdn.net/20140710141306611?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHVkaWVmZW5tdQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

##### HDFS读数据原理

![img](https://img-blog.csdn.net/20140711135958718?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHVkaWVmZW5tdQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

##### HDFS故障类型和其检测方法

![hdfs-tolerance-1.jpg](https://github.com/heibaiying/BigData-Notes/blob/master/pictures/hdfs-tolerance-1.jpg?raw=true)

![hdfs-tolerance-2.jpg](https://github.com/heibaiying/BigData-Notes/blob/master/pictures/hdfs-tolerance-2.jpg?raw=true)

**第二部分：读写故障的处理**

![img](https://raw.githubusercontent.com/heibaiying/BigData-Notes/master/pictures/hdfs-tolerance-3.jpg)

**第三部分：DataNode 故障处理**

![img](https://raw.githubusercontent.com/heibaiying/BigData-Notes/master/pictures/hdfs-tolerance-4.jpg)

**副本布局策略**：

![img](https://raw.githubusercontent.com/heibaiying/BigData-Notes/master/pictures/hdfs-tolerance-5.jpg)

#### HDFS网络拓扑

在本地网络中，两个节点被称为“彼此近邻”:

在海量数据处理中，其主要限制因素是节点之间数据的传输速率——带宽很稀缺。
这里的想法是将两个节点间的带宽作为距离的衡量标准。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20181104175640388.?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d3d3p5ZGNvbQ==,size_16,color_FFFFFF,t_70)



### MapReduce

#### 概述

Hadoop MapReduce 是一个分布式计算框架，用于编写批处理应用程序。

MapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 `map` 以并行的方式处理，框架对 `map` 的输出进行排序，然后输入到 `reduce` 中。MapReduce 框架专门用于 `` 键值对处理，它将作业的输入视为一组 `` 对，并生成一组 `` 对作为输出。输出和输出的 `key` 和 `value` 都必须实现[Writable](http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html) 接口。

```
(input) <k1, v1> -> map -> <k2, v2> -> combine -> <k2, v2> -> reduce -> <k3, v3> (output)
```

#### 特点

##### 优点

1. **易于编程**

    内部负责的操作，都是由框架本身去完成的，留给我们开发的内容少之又少（逻辑）

2. **良好的扩展性**

    随着我们业务或者需求的增加，集群的存储能力或者计算能力，又可能下降，这样我们可以通过增加集群的节点来解决

3. **高容错性**

     比如说我们一个节点挂掉了，我们的数据是不可能丢失的，因为我们的数据都会有副本，从而保证我们数据的完整性和正确性

4.  适合PB级别以上，海量数据的离线运算

##### 缺点

1. **实时运算**

    不能对实时的数据，进行分析运算，无法完成毫秒级或者秒级内返回结果

2.  **流式运算**

    MapReduce的运算数据必须是静态的，也就是事先已经确定了，不能动态的变化

3. **DAG运算**

     DAG(Database Availability Group) 就是有向无环图！

   如果我们多个应用程序之间存在依赖关系，A应用程序的输入参数是B项目的一个输出！

   MapReduce就显得力不从心了！因为他的计算速度比较慢，计算出来的中间结果等等都会写到磁盘上，输出的结果也会到磁盘上，读写磁盘就意味着性能不能达到实时的要求。做历史数据的批处理比较好。但spark等实时框架计算过程是用内存的，中间结果也可以放在内存中，所以很快，也就能达到实时的要求了

   

#### 编程模型

![mapreduceProcess.png](https://github.com/heibaiying/BigData-Notes/blob/master/pictures/mapreduceProcess.png?raw=true)



**Input**:读取文本文件；

**Splitting**: 将文件按照行进行拆分，此时得到的 `K1` 行数，`V1` 表示对应行的文本内容；

**Mapping**:并行将每一行按照空格进行拆分，拆分得到的 `List(K2,V2)`，其中 `K2` 代表每一个单词，由于是做词频统计，所以 `V2` 的值为 1，代表出现 1 次；

**Shuffling**:由于 `Mapping` 操作可能是在不同的机器上并行处理的，所以需要通过 `shuffling` 将相同 `key` 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 `K2` 为每一个单词，`List(V2)` 为可迭代集合，`V2` 就是 Mapping 中的 V2；

**Reducing**:这里的案例是统计单词出现的总次数，所以 `Reducing` 对 `List(V2)` 进行归约求和操作，最终输出。

**Final result**: 把最终的结果输出到HDFS文件系统中

MapReduce 编程模型中 `splitting` 和 `shuffing` 操作都是由框架实现的，需要我们自己编程实现的只有 `mapping` 和 `reducing`，这也就是 MapReduce 这个称呼的来源。



#### 组件

##### Map阶段

###### 概述

Map阶段由一定数量的Map Task组成（具体有多少Map Task根据数据量来分配）

 

输入数据格式解析： InputFormat 默认实现类是TextInputFormat

输入数据的处理： Mapper

本地归约Reduce: Combiner  

数据分组：Partitioner



###### combiner

1. 每一个map可能会产生大量的输出，Combiner的作用就是在map端对输出先做一次合并，减少本地磁盘的IO
2. Combiner最基本是实现本地key的归并，Combiner具有类似本地的reduce功能。 
    如果不用Combiner，那么，所有的结果都是reduce完成，效率会相对低下。 
    使用Combiner，先完成的map会在本地聚合，提升速度，减少了reduce拉取的网络IO！

3. 也不是所有的程序都需要Combiner,求和适合，但是平均值就不行了，因为会影响到结果！所以Combiner只应该用于那种Reduce的输入key/value与输出key/value类型完全一致，且不影响最终结果的场景。

   

###### Partitioner

1. 决定了Map Task输出的每条数据交给哪个Reduce Task来处理

2. 默认实现是根据key的hash值进行取模，我们可以自定义

   Hash(key) mod R  <word,count>

   R指的是Reduce Task的数量

3. 很多时候需要我们设置Partitioner ,可以提高Reduce Task的执行效率

   比如说我们需要把用户请求 jd商城的地址 都交给一个Reduce Task来处理！

  4.Partitioner 的数量不能超过Reduce Task的数量，否则会报错！



##### Reduce阶段

数据远程拷贝

数据按照key排序

数据处理：Reduce

数据输出格式：OutputFormat 默认实现类是TextOutputFormat



#### 内部逻辑视图

###### 执行流程

1.我们存储在HDFS上的文件，会被切成多个Block(块)。每一块数据，默认就是对应一个split（分片），片是MapReduce中的概念；

2.每一个MapTask就会处理一个split（分片）；

3.每个分片的内容都会通过InputFormat转换成<key,value>形式的数据，然后调用mapper方法，输出key,value,所有分片的内容，并行处理，计算结果存储在本地磁盘;

4.shuffle远程拉取磁盘文件并按照hash进行排序;

5.reduce将计算完成的数据通过outputFormat传输到HDFS中



###### 文件分片InputSplit

主要工作：

 01.处理跨行问题

 02.将分片数据解析成key,value

 

###### TextInputFormat

\01. key是行在文件中的偏移量，value是行的内容

02.如果行被截断，则会读取下一个block的前几个内容，下一个block会忽略前几个内容



#### 阶段所执行的任务

##### JobTracker

\1.   管理所有的作业job，存储在master节点上

\2.   将作业分解成一系列的任务

\3.   将任务指派给TaskTracker

\4.   作业/任务监控，错误处理等



##### TaskTracker

\1.      存在slave节点上

\2.      运行Map Task和Reduce Task

\3.      与JobTracker交互，执行命令，汇报任务状态（心跳）

 

##### Map Task

\1.  Map 引擎，解析每条数据记录，传递给我们编写的map()

\2.  将map()的输出数据写入到本地磁盘（如果是map-only作业，可以直接写入到HDFS）



##### Reduce Task

\1. Reduce引擎，从Map Task上远程拉取输入数据

\2. 对数据进行排序

\3. 将数据按照分组传递给我们编写的reduce()

Map Task的输出数据就是Reduce Task的输入数据！



### YARN

#### 简介

 Hadoop2.0新增系统

  负责集群的资源管理和调度

  使得多种计算框架可以运行在一个集群中



#### 特点

   良好的扩展性、高可用性

​    对多种类型的应用程序进行统一管理和调度

​    自带了多种多用户调度器，适合共享集群环境



#### 架构

![img](https://github.com/heibaiying/BigData-Notes/raw/master/pictures/Figure3Architecture-of-YARN.png)

##### Client 

与MapReduce 1.x的Client类似，用户通过Client与YARN 交互，提交MapReduce作业，查询作业运行状态，管理作 业等。

##### ResourceManager

`ResourceManager` 通常在独立的机器上以后台进程的形式运行，它是整个集群资源的主要协调者和管理者。`ResourceManager` 负责给用户提交的所有应用程序分配资源，它根据应用程序优先级、队列容量、ACLs、数据位置等信息，做出决策，然后以共享的、安全的、多租户的方式制定分配策略，调度集群资源。

##### NodeManager

`NodeManager` 是 YARN 集群中的每个具体节点的管理者。主要负责该节点内所有容器的生命周期的管理，监视资源和跟踪节点健康。具体如下：

- 启动时向 `ResourceManager` 注册并定时发送心跳消息，等待 `ResourceManager` 的指令；
- 维护 `Container` 的生命周期，监控 `Container` 的资源使用情况；
- 管理任务运行时的相关依赖，根据 `ApplicationMaster` 的需要，在启动 `Container` 之前将需要的程序及其依赖拷贝到本地。

##### ApplicationMaster

在用户提交一个应用程序时，YARN 会启动一个轻量级的进程 `ApplicationMaster`。`ApplicationMaster` 负责协调来自 `ResourceManager` 的资源，并通过 `NodeManager` 监视容器内资源的使用情况，同时还负责任务的监控与容错。具体如下：

- 根据应用的运行状态来决定动态计算资源需求；
- 向 `ResourceManager` 申请资源，监控申请的资源的使用情况；
- 跟踪任务状态和进度，报告资源的使用情况和应用的进度信息；
- 负责任务的容错。

##### Contain

`Container` 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源是用 `Container` 表示的。YARN 会为每个任务分配一个 `Container`，该任务只能使用该 `Container` 中描述的资源。`ApplicationMaster` 可在 `Container` 内运行任何类型的任务。例如，`MapReduce ApplicationMaster` 请求一个容器来启动 map 或 reduce 任务，而 `Giraph ApplicationMaster` 请求一个容器来运行 Giraph 任务。



#### 执行流程

![img](https://github.com/heibaiying/BigData-Notes/raw/master/pictures/yarn%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E7%AE%80%E5%9B%BE.png)

1. `Client` 提交作业到 YARN 上；
2. `Resource Manager` 选择一个 `Node Manager`，启动一个 `Container` 并运行 `Application Master` 实例；
3. `Application Master` 根据实际需要向 `Resource Manager` 请求更多的 `Container` 资源（如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务）；
4. `Application Master` 通过获取到的 `Container` 资源执行分布式计算。



#### 详细执行

![img](https://github.com/heibaiying/BigData-Notes/raw/master/pictures/yarn%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.png)



#####  作业提交

client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业 (第 1 步) 。新的作业 ID(应用 ID) 由资源管理器分配 (第 2 步)。作业的 client 核实作业的输出, 计算输入的 split, 将作业的资源 (包括 Jar 包，配置文件, split 信息) 拷贝给 HDFS(第 3 步)。 最后, 通过调用资源管理器的 submitApplication() 来提交作业 (第 4 步)。

##### 作业初始化

当资源管理器收到 submitApplciation() 的请求时, 就将该请求发给调度器 (scheduler), 调度器分配 container, 然后资源管理器在该 container 内启动应用管理器进程, 由节点管理器监控 (第 5 步)。

MapReduce 作业的应用管理器是一个主类为 MRAppMaster 的 Java 应用，其通过创造一些 bookkeeping 对象来监控作业的进度, 得到任务的进度和完成报告 (第 6 步)。然后其通过分布式文件系统得到由客户端计算好的输入 split(第 7 步)，然后为每个输入 split 创建一个 map 任务, 根据 mapreduce.job.reduces 创建 reduce 任务对象。

#####  任务分配

如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务。

如果不是小作业, 那么应用管理器向资源管理器请求 container 来运行所有的 map 和 reduce 任务 (第 8 步)。这些请求是通过心跳来传输的, 包括每个 map 任务的数据位置，比如存放输入 split 的主机名和机架 (rack)，调度器利用这些信息来调度任务，尽量将任务分配给存储数据的节点, 或者分配给和存放输入 split 的节点相同机架的节点。

##### 任务运行

当一个任务由资源管理器的调度器分配给一个 container 后，应用管理器通过联系节点管理器来启动 container(第 9 步)。任务由一个主类为 YarnChild 的 Java 应用执行， 在运行任务之前首先本地化任务需要的资源，比如作业配置，JAR 文件, 以及分布式缓存的所有文件 (第 10 步。 最后, 运行 map 或 reduce 任务 (第 11 步)。

YarnChild 运行在一个专用的 JVM 中, 但是 YARN 不支持 JVM 重用。

##### 进度和状态更新

YARN 中的任务将其进度和状态 (包括 counter) 返回给应用管理器, 客户端每秒 (通 mapreduce.client.progressmonitor.pollinterval 设置) 向应用管理器请求进度更新, 展示给用户。

#####  作业完成

除了向应用管理器请求作业进度外, 客户端每 5 分钟都会通过调用 waitForCompletion() 来检查作业是否完成，时间间隔可以通过 mapreduce.client.completion.pollinterval 来设置。作业完成之后, 应用管理器和 container 会清理工作状态， OutputCommiter 的作业清理方法也会被调用。作业的信息会被作业历史服务器存储以备之后用户核查。

## Hadoop搭建

### 单机

#### 1、Linux安装JDK

[https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/Linux%E4%B8%8BJDK%E5%AE%89%E8%A3%85.md](https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/Linux下JDK安装.md)

#### 2、配置免密登录

##### 2.1 配置映射

配置 ip 地址和主机名映射：

```
vim /etc/hosts
# 文件末尾增加
192.168.43.202  hadoop001
```

##### 2.2 生成公私钥

执行下面命令行生成公匙和私匙：

```
ssh-keygen -t rsa
```

##### 2.3 授权

进入 `~/.ssh` 目录下，查看生成的公匙和私匙，并将公匙写入到授权文件：

```
[root@@hadoop001 sbin]#  cd ~/.ssh
[root@@hadoop001 .ssh]# ll
-rw-------. 1 root root 1675 3 月  15 09:48 id_rsa
-rw-r--r--. 1 root root  388 3 月  15 09:48 id_rsa.pub
# 写入公匙到授权文件
[root@hadoop001 .ssh]# cat id_rsa.pub >> authorized_keys
[root@hadoop001 .ssh]# chmod 600 authorized_keys
```

#### 3、环境搭建

#####  下载并解压

http://archive.cloudera.com/cdh5/cdh/5/

```
# 解压
tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz 
```

```
配置环境变量
# vi /etc/profile

export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2
export  PATH=${HADOOP_HOME}/bin:$PATH

生效文件
# source /etc/profile
```

进入 `${HADOOP_HOME}/etc/hadoop/`

##### core-site.xml

```
<configuration>
    <property>
        <!--指定 namenode 的 hdfs 协议文件系统的通信地址-->
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop001:8020</value>
    </property>
    <property>
        <!--指定 hadoop 存储临时文件的目录-->
        <name>hadoop.tmp.dir</name>
        <value>/home/hadoop/tmp</value>
    </property>
</configuration>
```

##### hdfs-site.xml

指定副本系数和临时文件存储位置：

```
<configuration>
    <property>
        <!--由于我们这里搭建是单机版本，所以指定 dfs 的副本系数为 1-->
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```

##### slaves

配置所有从属节点的主机名或 IP 地址，由于是单机版本，所以指定本机即可：

```
hadoop001
```

#####  关闭防火墙

不关闭防火墙可能导致无法访问 Hadoop 的 Web UI 界面：

```
# 查看防火墙状态
sudo firewall-cmd --state
# 关闭防火墙:
sudo systemctl stop firewalld.service
```

##### 初始化

第一次启动 Hadoop 时需要进行初始化，进入 `${HADOOP_HOME}/bin/` 目录下，执行以下命令：

```
[root@hadoop001 bin]# ./hdfs namenode -format
```

#####  启动HDFS

进入 `${HADOOP_HOME}/sbin/` 目录下，启动 HDFS：

```
[root@hadoop001 sbin]# ./start-dfs.sh
```

#####  验证是否启动成功

方式一：执行 `jps` 查看 `NameNode` 和 `DataNode` 服务是否已经启动：

```
[root@hadoop001 hadoop-2.6.0-cdh5.15.2]# jps
9137 DataNode
9026 NameNode
9390 SecondaryNameNode
```

#### 4、Hadoop(YARN)环境搭建

进入 `${HADOOP_HOME}/etc/hadoop/ `目录下，修改以下配置：

##### 1. mapred-site.xml

```
# 如果没有mapred-site.xml，则拷贝一份样例文件后再修改
cp mapred-site.xml.template mapred-site.xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>
```

##### 2. yarn-site.xml

```
<configuration>
    <property>
        <!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。-->
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

##### 4.2 启动服务

进入 `${HADOOP_HOME}/sbin/` 目录下，启动 YARN：

```
./start-yarn.sh
```

##### 4.3 验证是否启动成功

方式一：执行 `jps` 命令查看 `NodeManager` 和 `ResourceManager` 服务是否已经启动：

```
[root@hadoop001 hadoop-2.6.0-cdh5.15.2]# jps
9137 DataNode
9026 NameNode
12294 NodeManager
12185 ResourceManager
9390 SecondaryNameNode
```

方式二：查看 Web UI 界面，端口号为 `8088`：

[![img](https://github.com/heibaiying/BigData-Notes/raw/master/pictures/hadoop-yarn%E5%AE%89%E8%A3%85%E9%AA%8C%E8%AF%81.png)](https://github.com/heibaiying/BigData-Notes/blob/master/pictures/hadoop-yarn安装验证.png)



### 集群

#### 一：下载安装 Hadoop

##### 1.1：下载指定的Hadoop

hadoop-2.8.0.tar.gz

 

##### 1.2：通过XFTP把文件上传到master电脑bigData目录下



##### 1.3：解压hadoop压缩文件

 tar -xvf hadoop-2.8.0.tar.gz

 

##### 1.4：进入压缩文件之后 复制路径

/bigData/hadoop-2.8.0



##### 1.5：配置Hadoop的环境变量

 

vim /etc/profile

 

export HADOOP_HOME=/bigData/hadoop-2.8.0

export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

 

之后让文件生效

source  /etc/profile 

 

#### 二：Hadoop集群的配置

##### 2.1:进入hadoop的配置文件位置

 cd  hadoop2.8.0/etc/hadoop/

##### 2.2:配置hadoop-env.sh文件 

​    01.vim hadoop-env.sh 

​    02.在文件中加入

​      export JAVA_HOME=/bigData/jdk1.8.0_121



##### 2.3:配置yarn-env.sh文件

​    01.vim yarn-env.sh

​    02.在文件中加入

​      export JAVA_HOME=/bigData/jdk1.8.0_121



##### 2.4:配置slaves文件，增加slave主机名或者IP地址

01.vim slaves

 

​    02.在文件中加入

​      slave1

​      slave2 ， 删除原有的localhost



##### 2.5:配置core-site.xml文件

 01.vim core-site.xml

 

 

​    02.在文件中的configuration节点里加入

<property>

  <name>fs.defaultFS</name>

  <value>hdfs://master:9000</value>

</property>

 

 <property>

   <name>hadoop.tmp.dir</name>

   <value>/bigData/hadoop-2.8.0/tmp</value> 

</property>

 

##### 2.6:配置hdfs-site.xml文件

<property>

   <name>dfs.namenode.secondary.http-address</name>

   <value>master:50090</value>

  </property>

  <property>

   <name>dfs.replication</name>

   <value>2</value>

  </property>

  <property>

   <name>dfs.namenode.name.dir</name>

   <value>file:/bigData/hadoop-2.8.0/hdfs/name</value>

  </property>

  <property>

   <name>dfs.datanode.data.dir</name>

   <value>file:/bigData/hadoop-2.8.0/hdfs/data</value>

  </property>

 

##### 2.7:配置yarn-site.xml文件

 

 <property>

​     <name>yarn.nodemanager.aux-services</name>

​     <value>mapreduce_shuffle</value>

 </property>

 <property>

​      <name>yarn.resourcemanager.address</name>

​      <value>master:8032</value>

 </property>

 <property>

​     <name>yarn.resourcemanager.scheduler.address</name>

​     <value>master:8030</value>

 </property>

<property> 

  <name>yarn.log-aggregation-enable</name> 

  <value>true</value> 

</property> 

<property>

​     <name>yarn.resourcemanager.resource-tracker.address</name>

​     <value>master:8031</value>

   </property>

   <property>

​     <name>yarn.resourcemanager.admin.address</name>

​     <value>master:8033</value>

   </property>

   <property>

​     <name>yarn.resourcemanager.webapp.address</name>

​     <value>master:8088</value>

   </property>

 

##### 2.8:配置mapred-site.xml文件

 

mapred-site.xml.template 是存在的

mapred-site.xml不存在

注意：先要copy一份

cp mapred-site.xml.template mapred-site.xml

然后编辑

vim mapred-site.xml 

 

 

 

 

新增以下内容

<property>

  <name>mapreduce.framework.name</name>

  <value>yarn</value>

 </property>

 <property>

  <name>mapreduce.jobhistory.address</name>

  <value>master:10020</value>

 </property>

 <property>

  <name>mapreduce.jobhistory.webapp.address</name>

  <value>master:19888</value>

 </property>

 

##### 2.9:把配置好的hadoop文件复制到其他的子机器中

 

scp -r /bigData/hadoop-2.8.0 root@slave1:/bigData/

 

scp -r /bigData/hadoop-2.8.0 root@slave2:/bigData/

 

##### 2.10:把配置好的/etc/profile复制到其他两个子机器中

 

scp /etc/profile root@slave1:/etc/profile

scp /etc/profile root@slave2:/etc/profile

 

之后在每个子机器中使用 source /etc/profile 使文件生效

 

##### 2.11:在master 主机器中运行 

 hdfs namenode -format

 

 

##### 2.12:在master 主机器中启动hadoop环境

 进入/bigData/hadoop-2.8.0/sbin

 

 ./start-all.sh  启动hadoop集群

 

./stop-all.sh 关闭hadoop集群

## HadoopShell命令

### **1. 显示当前目录结构**

```
# 显示当前目录结构
hadoop fs -ls  <path>
# 递归显示当前目录结构
hadoop fs -ls  -R  <path>
# 显示根目录下内容
hadoop fs -ls  /
```

### **2. 创建目录**

```
# 创建目录
hadoop fs -mkdir  <path> 
# 递归创建目录
hadoop fs -mkdir -p  <path>  
```

### **3. 删除操作**

```
# 删除文件
hadoop fs -rm  <path>
# 递归删除目录和文件
hadoop fs -rm -R  <path> 
```

### **4. 从本地加载文件到 HDFS**

```
# 二选一执行即可
hadoop fs -put  [localsrc] [dst] 
hadoop fs - copyFromLocal [localsrc] [dst] 
```

### **5. 从 HDFS 导出文件到本地**

```
# 二选一执行即可
hadoop fs -get  [dst] [localsrc] 
hadoop fs -copyToLocal [dst] [localsrc] 
```

### **6. 查看文件内容**

```
# 二选一执行即可
hadoop fs -text  <path> 
hadoop fs -cat  <path>  
```

### **7. 显示文件的最后一千字节**

```
hadoop fs -tail  <path> 
# 和Linux下一样，会持续监听文件内容变化 并显示文件的最后一千字节
hadoop fs -tail -f  <path> 
```

### **8. 拷贝文件**

```
hadoop fs -cp [src] [dst]
```

### **9. 移动文件**

```
hadoop fs -mv [src] [dst] 
```

### **10. 统计当前目录下各文件大小**

- 默认单位字节
- -s : 显示所有文件大小总和，
- -h : 将以更友好的方式显示文件大小（例如 64.0m 而不是 67108864）

```
hadoop fs -du  <path>  
```

### **11. 合并下载多个文件**

- -nl 在每个文件的末尾添加换行符（LF）
- -skip-empty-file 跳过空文件

```
hadoop fs -getmerge
# 示例 将HDFS上的hbase-policy.xml和hbase-site.xml文件合并后下载到本地的/usr/test.xml
hadoop fs -getmerge -nl  /test/hbase-policy.xml /test/hbase-site.xml /usr/test.xml
```

### **12. 统计文件系统的可用空间信息**

```
hadoop fs -df -h /
```

### **13. 更改文件复制因子**

```
hadoop fs -setrep [-R] [-w] <numReplicas> <path>
```

- 更改文件的复制因子。如果 path 是目录，则更改其下所有文件的复制因子
- -w : 请求命令是否等待复制完成

```
# 示例
hadoop fs -setrep -w 3 /user/hadoop/dir1
```

### **14. 权限控制**

```
# 权限控制和Linux上使用方式一致
# 变更文件或目录的所属群组。 用户必须是文件的所有者或超级用户。
hadoop fs -chgrp [-R] GROUP URI [URI ...]
# 修改文件或目录的访问权限  用户必须是文件的所有者或超级用户。
hadoop fs -chmod [-R] <MODE[,MODE]... | OCTALMODE> URI [URI ...]
# 修改文件的拥有者  用户必须是超级用户。
hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]
```

### **15. 文件检测**

```
hadoop fs -test - [defsz]  URI
```

可选选项：

- -d：如果路径是目录，返回 0。
- -e：如果路径存在，则返回 0。
- -f：如果路径是文件，则返回 0。
- -s：如果路径不为空，则返回 0。
- -r：如果路径存在且授予读权限，则返回 0。
- -w：如果路径存在且授予写入权限，则返回 0。
- -z：如果文件长度为零，则返回 0。

```
# 示例
hadoop fs -test -e filename
```

## HadoopAPI

### hdfs

```java
   private static Configuration conf;
    private static FileSystem fs;

    public static void HDFSClientStart() throws URISyntaxException, IOException, InterruptedException {
        System.setProperty("HADOOP_USER_NAME", "root");
        conf = new Configuration();
        fs = FileSystem.get(new URI("hdfs://10.254.1.36:8020"), conf);
    }


    /**
     * 判断文件还是文件夹
     */
    public static void isDirectory() throws IOException {
        boolean directory = fs.isDirectory(new Path("/home/14994.jpg"));
        System.out.println(directory);
    }

    /**
     * 文件删除
     */
    public static void deleteFile() throws IOException {
        boolean delete = fs.delete(new Path(" /home/14994.jpg"), true);
        System.out.println(delete);
    }


    /**本地上传文件*/
    public static void testUpload() throws IOException {
        System.out.println("hello");
        //将本地的文件上传到hdfs上面。第一个Path是本地的路径，第二个path是hdfs里面的路径。
        fs.copyFromLocalFile(new Path("e:/logs"),new Path("/mylog.txt"));
        fs.close();
        System.out.println("成功");
    }

    /** 下载文件 */
    public static void testDownload()throws IOException{
        //将hdfs里面的下载到本地中，第一个TRUE是代表删除源文件，false不删除源文件。第一个path是hdfs里面的路径；第二个path是本地的路径，ture是copy到本地系统，false是拷贝到hdfs系统中
        fs.copyToLocalFile(true,new Path("/mylog.txt"),new Path("d://copymylog.txt"),true);
        fs.close();
    }

    /**
     * 修改文件名
     */
    public void UpdateFileName() throws IOException {
        fs.rename(new Path("/test/wangzili"), new Path("/test/wzl"));
    }
```



### MapReduce

求一个文件中最大，最小和平均值

Mike,35
Steven,40
Ken,28
Cindy,32

#### Mapper

```java
public static class WordCountMapper extends Mapper<Object, Text, Text, Text> {
    private Text nameKey = new Text(); //记录姓名
    private Text ageValue = new Text();//记录年龄

    @Override
    public void map(Object key, Text value, Context context)
            throws IOException, InterruptedException {
//获取一行内容
        StringTokenizer itr = new StringTokenizer(value.toString());
        while (itr.hasMoreTokens()) {
            String content = itr.nextToken();
//以逗号作为分隔符
            String[] nameAndAge = content.split(",");
            String name = nameAndAge[0]; //保存姓名
            String age = nameAndAge[1]; // 保存年龄
            nameKey.set(name);
            ageValue.set(age); // 这里的key都不相同，所以reduce阶段会反复执行4次
            context.write(nameKey, ageValue); // 如果nameKey固定 就只会执行一次reduce
        }
    }

```



#### Reduce

```java
public static class WordCountReduce extends Reducer<Text, Text, Text, Text> {

    private int min = Integer.MAX_VALUE; //保存最小值
    private int max = 0; //保存最大值
    private int sum = 0;//保存总值
    private int count = 0;//保存平均值
    private String maxName; //记录最大值人员
    private String  minName; //记录最小值人员

    @Override
    public void reduce(Text key, Iterable<Text> values, Context context)
            throws IOException, InterruptedException {
        for (Text tmpAge : values) {
            int age = Integer.valueOf(tmpAge.toString());
            if (age < min) {
                min = age;
                minName=key.toString();//记录最小值的人员
            }
            if (age > max) {
                max = age;
                maxName=key.toString();//记录最大值的人员
            }
            sum += age;
            count++;
        }
        context.write(new Text(maxName+":最大值"), new Text(String.valueOf(max)));
        context.write(new Text(minName+":最小值"), new Text(String.valueOf(min)));
        context.write(new Text("平均值"),
                new Text(String.valueOf(sum / count)));
    }
}

```



#### Main

```java

    // 这里为了直观显示参数 使用了硬编码，实际开发中可以通过外部传参
    private static final String HDFS_URL = "hdfs://10.254.1.36:8020";
    private static final String HADOOP_USER_NAME = "root";

    public static void main(String[] args) throws IOException, URISyntaxException, InterruptedException, ClassNotFoundException {
        args = new String[]{"/home/test.txt","/home"};
        //  文件输入路径和输出路径由外部传参指定
        if (args.length < 2) {
            System.out.println("Input and output paths are necessary!");
            return;
        }


        // 需要指明 hadoop 用户名，否则在 HDFS 上创建目录时可能会抛出权限不足的异常
        System.setProperty("HADOOP_USER_NAME", HADOOP_USER_NAME);

        Configuration configuration = new Configuration();
        // 指明 HDFS 的地址
        configuration.set("fs.defaultFS", HDFS_URL);

        // 创建一个 Job
        Job job = Job.getInstance(configuration);

        // 设置运行的主类
        job.setJarByClass(WordCountApp.class);

        // 设置 Mapper 和 Reducer
        job.setMapperClass(WordCountMapper.class);
        job.setReducerClass(WordCountReducer.class);


        // 设置 Mapper 输出 key 和 value 的类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        // 设置 Reducer 输出 key 和 value 的类型
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);


        // 如果输出目录已经存在，则必须先删除，否则重复运行程序时会抛出异常
        FileSystem fileSystem = FileSystem.get(new URI(HDFS_URL), configuration, HADOOP_USER_NAME);
        Path outputPath = new Path(args[1]);
        if (fileSystem.exists(outputPath)) {
            fileSystem.delete(outputPath, true);
        }

        // 设置作业输入文件和输出文件的路径
        FileInputFormat.setInputPaths(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, outputPath);

        // 将作业提交到群集并等待它完成，参数设置为 true 代表打印显示对应的进度
        boolean result = job.waitForCompletion(true);

        // 关闭之前创建的 fileSystem
        fileSystem.close();

        // 根据作业结果,终止当前运行的 Java 虚拟机,退出程序
        System.exit(result ? 0 : -1);


```



## HadoopHA

[https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/%E5%9F%BA%E4%BA%8EZookeeper%E6%90%AD%E5%BB%BAHadoop%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4.md](https://github.com/heibaiying/BigData-Notes/blob/master/notes/installation/基于Zookeeper搭建Hadoop高可用集群.md)

## Hadoop学习资料

https://github.com/heibaiying/BigData-Notes